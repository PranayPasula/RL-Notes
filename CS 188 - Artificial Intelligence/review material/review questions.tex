\documentclass[]{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=-1mm, leftmargin=!}

%opening
\title{CS 188 -- Artificial Intelligence Notes (for Reinforcement Learning)}
\author{Pranay Pasula}

\begin{document}

\maketitle

\section*{Lecture 8: Markov Decision Processes I}
\begin{enumerate}
\item How is an MDP defined?
\item How does an MDP relate to a general search problem? \\
\item What is Markovian about MDPs?
\item What does this imply about how one should design an MDP? \\
\item What is usually the main goal in a deterministic search problem?
\item What is usually the main goal in an MDP? \\
\item What is a policy?
\item What does an explicit policy define? Explain what this is.
\item How does expectimax differ from finding an optimal policy?
\item How does expectimax relate to finding an optimal policy? \\
\item What is an MDP search tree?
\item How does an MDP search tree relate to an expectimax search tree?
\item How does an MDP search tree differ from an expectimax search tree? \\
\item What is discounting?
\item What issue may arise without discounting? What other condition must hold for this to be possible? \\
\item What are stationary preferences?
\item If we assume stationary preferences, what is implied about the ways to define utilities? \\
\item What is a potential issue if an MDP can last forever (and accrue infinite rewards)? What are three ways to address this? What are the two most common ways to address this? How does the third way relate to the probability of the MDP terminating? \\
\item What are the three optimal values of interest when solving MDPs? Explain the meaning of each one. \\
\item How many Q-values are associated with each state?
\item How do Q-values relate to values?
\item How can we find the value of a state?
\item What is the formula for the value of a state?
\item What is the formula for the Q-value of a pair $(s,a)$?
\item What are two potential issues with using just expectimax to solve an MDP (hint: consider an infinite-length MDP)? How can we address these? \\
\item What are time-limited values? How can we quickly compute $V_{i}(s)$ for any $i \in [0,1,...,k]$? In general, for what value of $i$ is $V_{i}(s) = V^{*}(s)$? \\
\item Explain the value iteration algorithm.
\item What is its computational complexity?
\item Is the solution unique?
\item Is the solution optimal?
\item What is the main problem with value iteration?
\item Does value iteration always converge?
\item How does the convergence of the values relate to the convergence of the policy?
\item Does value iteration converge to the optimal policy?
\item How does adjusting $\gamma$ affect the convergence rate? \\
\end{enumerate}

\section*{Lecture 9 - Markov Decision Processes II}
\begin{enumerate}[resume*]
\item What is policy evaluation?
\item What is a fixed policy?
\item Explain two ways to find the values for a fixed policy.
\item How does this compare to value iteration?
\item What is the complexity of the iterative approach to policy evaluation?
\item For the non-iterative approach, what kind of matrix is usually involved? (Hint: usually enables a computer to find the solution quickly) \\

\item Given the optimal value for each state, can we determine the optimal policy without any calculations?
\item Explain how to determine the optimal policy given optimal value for each state.
\item Explain how to determine the optimal policy given the Q-values for each state. \\

\item State three problems with value iteration.
\item How does the rate of changes of "max" actions differ for early, middle, and late stages of value iteration?

\item What are the two main steps of policy iteration? Explain each.
\item Does policy iteration converge to the optimal policy? Does it converge to the optimal values? Explain why.
\item Why use policy iteration instead of value iteration? For what type of MDP is policy iteration especially preferred over value iteration?
\item Does policy iteration result in a better solution than value iteration?
\item Explain how to find the optimal policy using value iteration.
\item Explain how to find the optimal values using policy iteration.
\item When does policy iteration terminate? How does $\gamma \in (0,1)$ affect this?

\item What is a multi-armed bandit?
\item What kind of rewards do choices in the multi-armed bandit problem usually have? How does this affect the corresponding MDP?
\item Explain how to find the optimal policy of a multi-armed bandit MDP given all of the details of the MDP. What is this called? State why this is advantageous to having only partial information of the MDP.
\item Explain how to find the optimal policy of a multi-armed bandit MDP given only partial details of the MDP. What is this called?




\end{enumerate}





\end{document}
