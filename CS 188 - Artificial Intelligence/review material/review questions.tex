\documentclass[]{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=-1mm, leftmargin=!}

%opening
\title{CS 188 -- Artificial Intelligence Notes (for Reinforcement Learning)}
\author{Pranay Pasula}

\begin{document}

\maketitle

\section*{Lecture 8: Markov Decision Processes I}
\begin{enumerate}
\item How is an MDP defined?
\item How does an MDP relate to a general search problem? \\
\item What is Markovian about MDPs?
\item What does this imply about how one should design an MDP? \\
\item What is usually the main goal in a deterministic search problem?
\item What is usually the main goal in an MDP? \\
\item What is a policy?
\item What does an explicit policy define? Explain what this is.
\item How does expectimax differ from finding an optimal policy?
\item How does expectimax relate to finding an optimal policy? \\
\item What is an MDP search tree?
\item How does an MDP search tree relate to an expectimax search tree?
\item How does an MDP search tree differ from an expectimax search tree? \\
\item What is discounting?
\item What issue may arise without discounting? What other condition must hold for this to be possible? \\
\item What are stationary preferences?
\item If we assume stationary preferences, what is implied about the ways to define utilities? \\
\item What is a potential issue if an MDP can last forever (and accrue infinite rewards)? What are three ways to address this? What are the two most common ways to address this? How does the third way relate to the probability of the MDP terminating? \\
\item What are the three optimal values of interest when solving MDPs? Explain the meaning of each one. \\
\item How many Q-values are associated with each state?
\item How do Q-values relate to values?
\item How can we find the value of a state?
\item What is the formula for the value of a state?
\item What is the formula for the Q-value of a pair $(s,a)$?
\item What are two potential issues with using just expectimax to solve an MDP (hint: consider an infinite-length MDP)? How can we address these? \\
\item What are time-limited values? How can we quickly compute $V_{i}(s)$ for any $i \in [0,1,...,k]$? In general, for what value of $i$ is $V_{i}(s) = V^{*}(s)$? \\
\item Explain the value iteration algorithm.
\item What is its computational complexity?
\item Is the solution unique?
\item Is the solution optimal?
\item What is the main problem with value iteration?
\item Does value iteration always converge?
\item How does the convergence of the values relate to the convergence of the policy?
\item Does value iteration converge to the optimal policy?
\item How does adjusting $\gamma$ affect the convergence rate? \\
\end{enumerate}

\section*{Lecture 9 - Markov Decision Processes II}



\end{document}
