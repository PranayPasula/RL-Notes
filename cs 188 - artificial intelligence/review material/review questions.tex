\documentclass[]{article}
\usepackage[margin=1.25in]{geometry}
\usepackage{enumitem}
\setlist[enumerate]{itemsep=-1mm, leftmargin=!}

%opening
\title{CS 188 -- Artificial Intelligence Notes (for Reinforcement Learning)}
\author{Pranay Pasula}

\begin{document}

\maketitle

\section*{Lecture 8: Markov Decision Processes I}
\begin{enumerate}
\item How is an MDP defined?
\item How does an MDP relate to a general search problem? \\
\item What is Markovian about MDPs?
\item What does this imply about how one should design an MDP? \\
\item What is usually the main goal in a deterministic search problem?
\item What is usually the main goal in an MDP? \\
\item What is a policy?
\item What does an explicit policy define? Explain what this is.
\item How does expectimax differ from finding an optimal policy?
\item How does expectimax relate to finding an optimal policy? \\
\item What is an MDP search tree?
\item How does an MDP search tree relate to an expectimax search tree?
\item How does an MDP search tree differ from an expectimax search tree? \\
\item What is discounting?
\item What issue may arise without discounting? What other condition must hold for this to be possible? \\
\item What are stationary preferences?
\item If we assume stationary preferences, what is implied about the ways to define utilities? \\
\item What is a potential issue if an MDP can last forever (and accrue infinite rewards)? What are three ways to address this? What are the two most common ways to address this? How does the third way relate to the probability of the MDP terminating? \\
\item What are the three optimal values of interest when solving MDPs? Explain the meaning of each one. \\
\item How many Q-values are associated with each state?
\item How do Q-values relate to values?
\item How can we find the value of a state?
\item What is the formula for the value of a state?
\item What is the formula for the Q-value of a pair $(s,a)$?
\item What are two potential issues with using just expectimax to solve an MDP (hint: consider an infinite-length MDP)? How can we address these? \\
\item What are time-limited values? How can we quickly compute $V_{i}(s)$ for any $i \in [0,1,...,k]$? In general, for what value of $i$ is $V_{i}(s) = V^{*}(s)$? \\
\item Explain the value iteration algorithm.
\item What is its computational complexity?
\item Is the solution unique?
\item Is the solution optimal?
\item What is the main problem with value iteration?
\item Does value iteration always converge?
\item How does the convergence of the values relate to the convergence of the policy?
\item Does value iteration converge to the optimal policy?
\item How does adjusting $\gamma$ affect the convergence rate? \\
\end{enumerate}

\section*{Lecture 9 - Markov Decision Processes II}
\begin{enumerate}
\item What is policy evaluation?
\item What is a fixed policy?
\item Explain two ways to find the values for a fixed policy.
\item How does this compare to value iteration?
\item What is the complexity of the iterative approach to policy evaluation?
\item For the non-iterative approach, what kind of matrix is usually involved? (Hint: usually enables a computer to find the solution quickly) \\

\item Given the optimal value for each state, can we determine the optimal policy without any calculations?
\item Explain how to determine the optimal policy given optimal value for each state.
\item Explain how to determine the optimal policy given the Q-values for each state. \\

\item State three problems with value iteration.
\item How does the rate of changes of "max" actions differ for early, middle, and late stages of value iteration?

\item What are the two main steps of policy iteration? Explain each.
\item Does policy iteration converge to the optimal policy? Does it converge to the optimal values? Explain why.
\item Why use policy iteration instead of value iteration? For what type of MDP is policy iteration especially preferred over value iteration?
\item Does policy iteration result in a better solution than value iteration?
\item Explain how to find the optimal policy using value iteration.
\item Explain how to find the optimal values using policy iteration.
\item When does policy iteration terminate? How does $\gamma \in (0,1)$ affect this?

\item What is a multi-armed bandit?
\item What kind of rewards do choices in the multi-armed bandit problem usually have? How does this affect the corresponding MDP?
\item Explain how to find the optimal policy of a multi-armed bandit MDP given all of the details of the MDP. What is this called? State why this is advantageous to having only partial information of the MDP.
\item Explain how to find the optimal policy of a multi-armed bandit MDP given only partial details of the MDP. What is this called?\\
\end{enumerate}

\section*{Lecture 10 - Reinforcement Learning I}

\begin{enumerate}
\item Describe the basic feedback loop of reinforcement learning.
\item What feedback is received by the agent?
\item What is the goal of the agent?
\item What does the agent learn from? \\

\item What is the goal of reinforcement learning?
\item How does a reinforcement learning problem differ from an MDP problem?\\

\item What is model-based learning? Explain how to do it.
\item How do the estimated values in model-based learning act as the number of experiences approaches infinity?
\item What is a disadvantage of model-based learning?\\

\item What is model-free learning?
\item What is an advantage of model-free learning over model-based learning?\\

\item What is passive reinforcement learning? What is the goal? How does this differ from offline planning?\\

\item What is direct evaluation? What are its advantages and disadvantages? \\

\item What is policy evaluation? Can we use it to solve the passive RL problem? Why or why not?\\

\item What is sample-based policy evaluation? Why may it not be suitable to solve the passive RL problem?\\

\item What is temporal difference learning? When do updates occur? How do updates occur? What info does this algorithm take into account that direct evaluation neglects?
\item What type of average does temporal difference learning use? How does this average behave? How does the parameter of this average affect the behavior? How can we adjust this type of average to promote convergence?
\item How does temporal difference learning relate to Bellman updates?
\item What is the main problem with temporal difference learning? What should we do to avoid this? \\

\item What is active reinforcement learning? What is its goal? 
\item How does active RL differ from passive RL?
\item What is the fundamental trade-off in active reinforcement learning?

\item Why can't we use value iteration to solve the active RL problem?
\item What is Q-value iteration? Why can't we use this to solve the active RL problem?
\item What is Q-learning? How does it relate to temporal difference learning? What conditions are required for it to converge to the optimal policy?
\item What is off-policy learning? Is Q-learning considered on-policy or off-policy? Explain why.\\
\end{enumerate}

\section*{Lecture 11 - Reinforcement Learning II}


\end{document}
